{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check if using cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"Covertype\"\n",
    "\n",
    "# bool_columns = [\n",
    "#     \"Wilderness_Area1\", \"Wilderness_Area2\", \"Wilderness_Area3\",\n",
    "#     \"Wilderness_Area4\", \"Soil_Type1\", \"Soil_Type2\", \"Soil_Type3\", \"Soil_Type4\",\n",
    "#     \"Soil_Type5\", \"Soil_Type6\", \"Soil_Type7\", \"Soil_Type8\", \"Soil_Type9\",\n",
    "#     \"Soil_Type10\", \"Soil_Type11\", \"Soil_Type12\", \"Soil_Type13\", \"Soil_Type14\",\n",
    "#     \"Soil_Type15\", \"Soil_Type16\", \"Soil_Type17\", \"Soil_Type18\", \"Soil_Type19\",\n",
    "#     \"Soil_Type20\", \"Soil_Type21\", \"Soil_Type22\", \"Soil_Type23\", \"Soil_Type24\",\n",
    "#     \"Soil_Type25\", \"Soil_Type26\", \"Soil_Type27\", \"Soil_Type28\", \"Soil_Type29\",\n",
    "#     \"Soil_Type30\", \"Soil_Type31\", \"Soil_Type32\", \"Soil_Type33\", \"Soil_Type34\",\n",
    "#     \"Soil_Type35\", \"Soil_Type36\", \"Soil_Type37\", \"Soil_Type38\", \"Soil_Type39\",\n",
    "#     \"Soil_Type40\"\n",
    "# ]\n",
    "\n",
    "# int_columns = [\n",
    "#     \"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
    "#     \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "#     \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "#     \"Horizontal_Distance_To_Fire_Points\"\n",
    "# ]\n",
    "\n",
    "# feature_columns = (\n",
    "#     int_columns + bool_columns + [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parit\\AppData\\Local\\Temp\\ipykernel_22188\\3119453846.py:3: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('data/synthetic_easy.csv', header=None)\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv(out, header=None, names=feature_columns)\n",
    "train = pd.read_csv('data/forest-cover-type.csv', header=None, names=feature_columns)\n",
    "# train = pd.read_csv('data/synthetic_easy.csv', header=None)\n",
    "\n",
    "n_total = len(train)\n",
    "\n",
    "# Train, val and test split follows\n",
    "# Rory Mitchell, Andrey Adinets, Thejaswi Rao, and Eibe Frank.\n",
    "# Xgboost: Scalable GPU accelerated learning. arXiv:1806.11248, 2018.\n",
    "\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    range(n_total), test_size=0.2, random_state=0)\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    train_val_indices, test_size=0.2 / 0.6, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "Label encode categorical features and fill empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = []\n",
    "# categorical_dims =  {}\n",
    "# for col in train.columns[train.dtypes == object]:\n",
    "#     print(col, train[col].nunique())\n",
    "#     l_enc = LabelEncoder()\n",
    "#     train[col] = train[col].fillna(\"VV_likely\")\n",
    "#     train[col] = l_enc.fit_transform(train[col].values)\n",
    "#     categorical_columns.append(col)\n",
    "#     categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "# for col in train.columns[train.dtypes == 'float64']:\n",
    "#     train.fillna(train.loc[train_indices, col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is a generic pipeline but actually no categorical features are available for this dataset\n",
    "\n",
    "# unused_feat = []\n",
    "\n",
    "# features = [ col for col in train.columns if col not in unused_feat+[target]] \n",
    "\n",
    "# cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "# cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"CI\", False):\n",
    "# Take only a subsample to run CI\n",
    "    X_train = train[features].values[train_indices][:1000,:]\n",
    "    y_train = train[target].values[train_indices][:1000]\n",
    "else:\n",
    "    X_train = train[features].values[train_indices]\n",
    "    y_train = train[target].values[train_indices]\n",
    "\n",
    "X_valid = train[features].values[valid_indices]\n",
    "y_valid = train[target].values[valid_indices]\n",
    "\n",
    "X_test = train[features].values[test_indices]\n",
    "y_test = train[target].values[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}\")\n",
    "print(f\"X_valid.shape: {X_valid.shape}, y_valid.shape: {y_valid.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNetPretrainer\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax', # \"sparsemax\",\n",
    "    n_shared_decoder=1, # nb shared glu for decoding\n",
    "    n_indep_decoder=1, # nb independent glu for decoding\n",
    "#     grouped_features=[[0, 1]], # you can group features together here\n",
    "    verbose=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100 if not os.getenv(\"CI\", False) else 2 # 1000\n",
    "# max_epochs = 10 if not os.getenv(\"CI\", False) else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# unsupervised_model.fit(\n",
    "#     X_train=X_train,\n",
    "#     eval_set=[X_valid],\n",
    "#     max_epochs=max_epochs , patience=5,\n",
    "#     batch_size=2048, virtual_batch_size=128,\n",
    "#     num_workers=0,\n",
    "#     drop_last=False,\n",
    "#     pretraining_ratio=0.5,\n",
    "# ) \n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train=X_train,\n",
    "    eval_set=[X_valid],\n",
    "    max_epochs=max_epochs , patience=0,\n",
    "    batch_size=2048, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    pretraining_ratio=0.5,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reconstruction from a dataset\n",
    "reconstructed_X, embedded_X = unsupervised_model.predict(X_valid)\n",
    "assert(reconstructed_X.shape==embedded_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain class-balanced samples from the training data\n",
    "train_balanced = train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=500, random_state=42))\n",
    "\n",
    "X_train_balanced = train_balanced[features].values\n",
    "y_train_balanced = train_balanced[target].values\n",
    "\n",
    "# Extract embeddings using the pretrained model\n",
    "_, embedded_X_train_balanced = unsupervised_model.predict(X_train_balanced)\n",
    "\n",
    "# Verify the shape of the extracted embeddings\n",
    "print(\"Embeddings shape:\", embedded_X_train_balanced.shape)\n",
    "print(\"Number of samples:\", len(y_train_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1)\n",
    "tsne_embeddings = tsne.fit_transform(embedded_X_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAT_library.iVAT import iVAT\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "pairwise_dist = cdist(tsne_embeddings, tsne_embeddings)\n",
    "\n",
    "RiV, RV, reordering_mat = iVAT(pairwise_dist)\n",
    "\n",
    "plt.imshow(RiV, cmap='gray')\n",
    "plt.savefig('tabnet_test3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_dist = cdist(embedded_X_train_balanced, embedded_X_train_balanced)\n",
    "\n",
    "RiV, RV, reordering_mat = iVAT(pairwise_dist)\n",
    "\n",
    "plt.imshow(RiV, cmap='gray')\n",
    "plt.savefig('tabnet_test3_raw_embeddings.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unsupervised_explain_matrix, unsupervised_masks = unsupervised_model.explain(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(unsupervised_masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load the same way as other TabNet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_model.save_model('./test_pretrain3')\n",
    "loaded_pretrain = TabNetPretrainer()\n",
    "loaded_pretrain.load_model('./test_pretrain3.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-3),\n",
    "                       scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='sparsemax', # This will be overwritten if using pretrain model\n",
    "                       verbose=5,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc'],\n",
    "    max_epochs=max_epochs , patience=20,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False,\n",
    "    from_unsupervised=loaded_pretrain,\n",
    "    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(clf.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot auc\n",
    "plt.plot(clf.history['train_auc'])\n",
    "plt.plot(clf.history['valid_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rates\n",
    "plt.plot(clf.history['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(X_test)\n",
    "test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "\n",
    "\n",
    "preds_valid = clf.predict_proba(X_valid)\n",
    "valid_auc = roc_auc_score(y_score=preds_valid[:,1], y_true=y_valid)\n",
    "\n",
    "print(f\"BEST VALID SCORE FOR {dataset_name} : {clf.best_cost}\")\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that best weights are used\n",
    "assert np.isclose(valid_auc, np.max(clf.history['valid_auc']), atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tabnet model\n",
    "saving_path_name = \"./tabnet_model_test_1\"\n",
    "saved_filepath = clf.save_model(saving_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new model with basic parameters and load state dict weights\n",
    "loaded_clf = TabNetClassifier()\n",
    "loaded_clf.load_model(saved_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_preds = loaded_clf.predict_proba(X_test)\n",
    "loaded_test_auc = roc_auc_score(y_score=loaded_preds[:,1], y_true=y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {loaded_test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(test_auc == loaded_test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global explainability : feat importance summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local explainability and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
